{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore, Experiment\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.core import Pipeline, PipelineRun\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print(f'WS name: {workspace.name}\\nRegion: {workspace.location}\\nSubscription id: {workspace.subscription_id}\\nResource group: {workspace.resource_group}')\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='offerallocation-parquet-default')\n",
    "#dataset.download(target_path='.', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydatastore = Datastore.get(workspace, 'workspaceblobstore')\n",
    "\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from  azureml.pipeline.core.graph import PipelineParameter\n",
    "data_path = DataPath(datastore=mydatastore, path_on_datastore='parquet/offerpath2')\n",
    "datapath1_pipeline_param = PipelineParameter(name=\"input_datapath\", default_value=data_path)\n",
    "datapath_input = (datapath1_pipeline_param, DataPathComputeBinding(mode='mount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StepToWriteDateFile created\n"
     ]
    }
   ],
   "source": [
    "StepToWriteDateFile = PythonScriptStep(\r\n",
    "    name='StepToWriteDateFile',\r\n",
    "    script_name=\"StepToWriteDateFile.py\",\r\n",
    "    arguments=[ \"--arg1\", datapath_input],\r\n",
    "    inputs=[datapath_input],\r\n",
    "    #runconfig = compute_config ,\r\n",
    "    compute_target='main-cluster', \r\n",
    "    source_directory='.')\r\n",
    "print(\"StepToWriteDateFile created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting StepToWriteDateFile.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile StepToWriteDateFile.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--arg1\", type=str, help=\"sample datapath argument\")\n",
    "import shutil\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "root_dir = args.arg1\n",
    "\n",
    "file_set = set()\n",
    "\n",
    "print(f'my root dir inside this script is: {root_dir}')\n",
    "\n",
    "counter = 0\n",
    "if os.path.exists(os.path.join(args.arg1,\"FileToProcess\")):\n",
    "    shutil.rmtree(os.path.join(args.arg1,\"FileToProcess\"))\n",
    "os.makedirs(os.path.join(args.arg1,\"FileToProcess\"))\n",
    "    \n",
    "for dir_, _, files in os.walk(root_dir):\n",
    "    for file_name in files:\n",
    "        # can do more fancy logic here and control what gets passed on for next step\n",
    "        if(file_name[-7:] == \"parquet\"):\n",
    "            rel_dir = os.path.relpath(dir_, root_dir)\n",
    "            rel_file = os.path.join(rel_dir, file_name)\n",
    "            mydf = pd.DataFrame({'dir':[rel_dir], 'fileName':[file_name],'fullFileName':[rel_file]})\n",
    "            fileWriteLocation = os.path.join(args.arg1,\"FileToProcess\",f\"file{counter}.csv\")\n",
    "            mydf.to_csv(fileWriteLocation)\n",
    "            counter = counter + 1\n",
    "            file_set.add(rel_file)\n",
    "print(file_set)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run,Environment\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "output_dir = OutputFileDatasetConfig(name=\"scores\")\n",
    "\n",
    "dataset = Dataset.File.from_files(path = [(mydatastore, \"parquet/offerpath2/FileToProcess/*\")])\n",
    "\n",
    "env = Environment(name=\"parallelenv\")\n",
    "\n",
    "env.from_conda_specification('parallelenv','parallelenv.yml')\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "   source_directory='.',\n",
    "   entry_script='ParallelModelTrain.py',\n",
    "   mini_batch_size=\"1\",\n",
    "   error_threshold=2,\n",
    "   output_action=\"append_row\",\n",
    "   environment=env,\n",
    "   compute_target='main-cluster',\n",
    "   append_row_file_name=\"my_outputs.txt\",\n",
    "   run_invocation_timeout=1200,\n",
    "   node_count=1)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "   name=\"paralleltrainmodels\",\n",
    "   parallel_run_config=parallel_run_config,\n",
    "   inputs=[dataset.as_named_input(\"inputds\") ],\n",
    "   output=output_dir\n",
    "   #models=[ model ] #not needed as its only relevant in batch inferencing\n",
    "   #arguments=[ ],\n",
    "   #allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ParallelModelTrain.py\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "from azureml.core import Workspace, Datastore, Experiment,Dataset, Experiment, Run\r\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\r\n",
    "\r\n",
    "\r\n",
    "def init():\r\n",
    "    global workspace\r\n",
    "    global dataset\r\n",
    "    global mounted_path\r\n",
    "    print('beginning the process')\r\n",
    "    workspace_name = 'tsi_ml_path'\r\n",
    "    run = Run.get_context()\r\n",
    "    client_secret = run.get_secret(name=\"SECRETNAME\") #you insert the secret name you have created\r\n",
    "    subscription_id = '' #your subscription id\r\n",
    "    resource_group = ''   #name of the resource group of your workspace\r\n",
    "    svc_pr = ServicePrincipalAuthentication(tenant_id=\"\",service_principal_id=\"\",service_principal_password=\"client_secret\")\r\n",
    "    workspace = Workspace(subscription_id, resource_group, workspace_name, auth=svc_pr)\r\n",
    "    dataset = Dataset.get_by_name(workspace, name='offerallocation-parquet-default')\r\n",
    "    import tempfile\r\n",
    "    mounted_path = tempfile.mkdtemp()\r\n",
    "    # mount dataset onto the mounted_path of a Linux-based compute\r\n",
    "    mount_context = dataset.mount(mounted_path)\r\n",
    "    mount_context.start()\r\n",
    " \r\n",
    "\r\n",
    "def run(mini_batch):\r\n",
    "\r\n",
    "    for file_path in mini_batch:\r\n",
    "        print (f\"in mini batch process {mounted_path}\")\r\n",
    "        input_data = pd.read_csv(file_path)\r\n",
    "        for index, row in input_data.iterrows():\r\n",
    "            print('row is as follows')\r\n",
    "            print(row)\r\n",
    "            print(os.path.join(mounted_path,row['fullFileName']))\r\n",
    "            modelinputdata = pd.read_parquet(os.path.join(mounted_path,row['fullFileName']))\r\n",
    "            print(f\"model input data shape is {modelinputdata.shape}\")\r\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineRun,StepSequence\n",
    "step_sequence = StepSequence(steps=[StepToWriteDateFile])# , parallelrun_step])\n",
    "pipeline = Pipeline(workspace=workspace, steps=step_sequence)\n",
    "print(\"pipeline with the train_steps created\")\n",
    "experiment_name = 'parallelstepwork'\n",
    "source_directory  = '.'\n",
    "\n",
    "experiment = Experiment(workspace, experiment_name)\n",
    "pipeline_run = experiment.submit(pipeline,pipeline_parameters={\"input_datapath\":data_path})\n",
    "#pipeline.publish(name='pipeline to parallel file processing', description='show parallel processing', version=\"1.0\", continue_on_step_failure=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will create an endpoint for your pipeline which can be called from your ADF\r\n",
    "\r\n",
    "#published_pipeline = pipeline.publish(name=\"ms-datapath-prs\", description=\"Pipeline to test Datapath and PRS\", version=\"1.0\", continue_on_step_failure=True)\r\n",
    "#published_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0a8f2a2d2cdd8369d479e36c37e91a13dec6c73420537bb663ad6c08c81644add"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}